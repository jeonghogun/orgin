"""
RAG (Retrieval-Augmented Generation) Service
"""

import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from app.services.external_api_service import ExternalSearchService
from app.services.llm_service import LLMService
from app.services.memory_service import MemoryService
from app.services.storage_service import StorageService
from app.models.memory_schemas import ConversationContext, UserProfile, MemoryEntry

logger = logging.getLogger(__name__)


@dataclass
class RAGContext:
    """RAG ì»¨í…ìŠ¤íŠ¸ ì •ë³´"""

    user_profile: Optional[UserProfile]
    conversation_context: Optional[ConversationContext]
    search_results: List[Dict[str, str]]
    wiki_summary: Optional[str]
    relevant_memories: List[MemoryEntry]
    user_query: str
    intent: str
    entities: Dict[str, str]


class RAGService:
    """RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        search_service: ExternalSearchService,
        llm_service: LLMService,
        memory_service: MemoryService,
        storage_service: StorageService,
    ):
        super().__init__()
        self.search_service = search_service
        self.llm_service = llm_service
        self.memory_service = memory_service
        self.storage_service = storage_service

    async def generate_rag_response(
        self,
        room_id: str,
        user_id: str,
        user_message: str,
        intent: str,
        entities: Dict[str, str],
        request_id: str,
    ) -> str:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        try:
            # 1. ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìƒì† ë¡œì§ ì¶”ê°€)
            rag_context = await self._collect_context(
                room_id, user_id, user_message, intent, entities
            )

            # 2. ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ (í•„ìš”ì‹œ)
            await self._enhance_with_external_data(rag_context)

            # 3. RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            rag_prompt = self._build_rag_prompt(rag_context)

            # 4. LLM ì‘ë‹µ ìƒì„±
            response = await self._generate_llm_response(rag_prompt, request_id)

            # 5. ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            await self._update_context_after_rag_response(
                room_id, user_id, user_message, response, rag_context
            )

            return response

        except Exception as e:
            logger.error(f"Failed to generate RAG response: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. RAG ê¸°ë°˜ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async def _collect_context(
        self,
        room_id: str,
        user_id: str,
        user_message: str,
        intent: str,
        entities: Dict[str, str],
    ) -> RAGContext:
        """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ ìƒì† ë¡œì§ í¬í•¨)"""
        # Get current room to check its type and parent
        current_room = await self.storage_service.get_room(room_id)
        if not current_room:
            # This should ideally not happen if called from a valid context
            raise ValueError(f"Room with id {room_id} not found.")

        # Always fetch the user's general profile
        user_profile = await self.memory_service.get_user_profile(user_id)

        # --- Memory Inheritance and Retrieval Logic ---
        room_ids_to_search = [room_id]
        if current_room.type == "sub" and current_room.parent_id:
            logger.info(f"Sub room {room_id} inheriting memory from main room {current_room.parent_id}")
            room_ids_to_search.append(current_room.parent_id)

        # Fetch relevant memories from all specified rooms in a single query
        relevant_memories = await self.memory_service.get_relevant_memories(
            room_ids=room_ids_to_search,
            user_id=user_id,
            query_text=user_message,
            limit=5,  # Fetch a slightly larger pool of memories
        )

        # Fetch conversation context (summary) for the current room
        conversation_context = await self.memory_service.get_context(room_id, user_id)

        # If it's a sub-room, fetch and prepend the parent's context summary
        if current_room.type == "sub" and current_room.parent_id:
            parent_context = await self.memory_service.get_context(current_room.parent_id, user_id)
            if parent_context and parent_context.summary:
                if conversation_context and conversation_context.summary:
                    conversation_context.summary = (
                        f"Parent Room Context: {parent_context.summary}\n\n"
                        f"Current Room Context: {conversation_context.summary}"
                    )
                elif conversation_context:
                    conversation_context.summary = f"Parent Room Context: {parent_context.summary}"

        return RAGContext(
            user_profile=user_profile,
            conversation_context=conversation_context,
            search_results=[],
            wiki_summary=None,
            relevant_memories=relevant_memories,
            user_query=user_message,
            intent=intent,
            entities=entities,
        )

    async def _enhance_with_external_data(self, rag_context: RAGContext) -> None:
        """ì™¸ë¶€ ë°ì´í„°ë¡œ ì»¨í…ìŠ¤íŠ¸ ê°•í™”"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°)
            if self._needs_search(rag_context):
                query = self._extract_search_query(rag_context)
                if query:
                    # ë” ë§ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                    raw_results = await self.search_service.web_search(query, 10)
                    filtered_results = self._filter_and_rank_search_results(
                        raw_results, query
                    )
                    rag_context.search_results = filtered_results[:3]  # ìµœì¢… 3ê°œë§Œ ì‚¬ìš©

            # ìœ„í‚¤ ì •ë³´ ì¶”ê°€ (íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°)
            if self._needs_wiki(rag_context):
                topic = self._extract_wiki_topic(rag_context)
                if topic:
                    wiki_summary = await self.search_service.wiki_summary(topic)
                    if wiki_summary:
                        rag_context.wiki_summary = self._validate_wiki_content(
                            wiki_summary
                        )

        except Exception as e:
            logger.error(f"Failed to enhance with external data: {e}")

    def _needs_search(self, rag_context: RAGContext) -> bool:
        """ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨"""
        # ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
        search_keywords = [
            "ìµœì‹ ",
            "ìµœê·¼",
            "í˜„ì¬",
            "ì–´ë–»ê²Œ",
            "ë¬´ì—‡",
            "ì–´ë””",
            "ì–¸ì œ",
            "ì™œ",
            "ë°©ë²•",
            "ê¸°ë²•",
            "íŠ¸ë Œë“œ",
            "ë™í–¥",
            "ë‰´ìŠ¤",
            "ì •ë³´",
        ]

        query_lower = rag_context.user_query.lower()
        return any(keyword in query_lower for keyword in search_keywords)

    def _needs_wiki(self, rag_context: RAGContext) -> bool:
        """ìœ„í‚¤ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨"""
        # íŠ¹ì • ì£¼ì œë‚˜ ê°œë…ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
        wiki_keywords = [
            "ë¬´ì—‡",
            "ì •ì˜",
            "ê°œë…",
            "ì—­ì‚¬",
            "ë°œì „",
            "ì›ë¦¬",
            "êµ¬ì¡°",
            "íŠ¹ì§•",
            "ì¥ì ",
            "ë‹¨ì ",
            "ë¹„êµ",
            "ë¶„ë¥˜",
        ]

        query_lower = rag_context.user_query.lower()
        return any(keyword in query_lower for keyword in wiki_keywords)

    def _extract_search_query(self, rag_context: RAGContext) -> Optional[str]:
        """ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ"""
        # ê¸°ë³¸ ì‚¬ìš©ì ì¿¼ë¦¬
        base_query = rag_context.user_query.strip()

        # ë§¥ë½ ì •ë³´ ìˆ˜ì§‘
        context_parts: List[str] = []

        # ì´ì „ ëŒ€í™” ìš”ì•½ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if (
            rag_context.conversation_context
            and rag_context.conversation_context.summary
        ):
            context_parts.append(rag_context.conversation_context.summary)

        # ì£¼ìš” ì£¼ì œê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if (
            rag_context.conversation_context
            and rag_context.conversation_context.key_topics
        ):
            context_parts.extend(rag_context.conversation_context.key_topics)

        # ê´€ë ¨ ë©”ëª¨ë¦¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if rag_context.relevant_memories:
            for memory in rag_context.relevant_memories:
                context_parts.append(f"{memory.key}: {memory.value}")

        # ë§¥ë½ì„ í¬í•¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        if context_parts:
            context_str = " ".join(context_parts)
            # ë§¥ë½ + í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            enhanced_query = f"{context_str} {base_query}"
        else:
            enhanced_query = base_query

        # ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        stop_words = [
            "ì´",
            "ê°€",
            "ì„",
            "ë¥¼",
            "ì˜",
            "ì—",
            "ë¡œ",
            "ìœ¼ë¡œ",
            "ì™€",
            "ê³¼",
            "ë„",
            "ë§Œ",
            "ì€",
            "ëŠ”",
        ]
        for word in stop_words:
            enhanced_query = enhanced_query.replace(word, " ")

        # ì¿¼ë¦¬ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
        enhanced_query = enhanced_query.strip()
        if len(enhanced_query) > 200:
            # í˜„ì¬ ì§ˆë¬¸ì„ ìš°ì„ í•˜ê³ , ë§¥ë½ì€ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            words = enhanced_query.split()
            if len(words) > 30:
                enhanced_query = " ".join(words[:30])

        return enhanced_query if len(enhanced_query) > 2 else None

    def _extract_wiki_topic(self, rag_context: RAGContext) -> Optional[str]:
        """ìœ„í‚¤ ì£¼ì œ ì¶”ì¶œ"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ìœ„í‚¤ ê²€ìƒ‰í•  ì£¼ì œ ì¶”ì¶œ
        query = rag_context.user_query.strip()

        # ëª…ì‚¬ë‚˜ ì£¼ì œì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ì‚¬ìš© ê°€ëŠ¥
        words = query.split()
        if len(words) >= 2:
            return words[0] + " " + words[1]  # ì²« ë‘ ë‹¨ì–´ ì¡°í•©
        elif len(words) == 1:
            return words[0]

        return None

    def _build_rag_prompt(self, rag_context: RAGContext) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt_parts: List[str] = []

        # ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
        prompt_parts.append(
            (
                "ë‹¹ì‹ ì€ ì™¸ë¶€ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ë˜, ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”:\n"
                "1. ê²€ìƒ‰ ê²°ê³¼ë‚˜ ìœ„í‚¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•˜ì„¸ìš”\n"
                "2. ì •ë³´ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”\n"
                "3. ìµœì‹  ì •ë³´ì„ì„ ê°•ì¡°í•˜ê³ , ì¶œì²˜ëŠ” 1-2ê°œë§Œ ëª…ì‹œí•˜ì„¸ìš”\n"
                "4. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”\n"
                "5. ì‚¬ìš©ìì˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n"
                "6. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³ , í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”"
            )
        )

        # ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´
        if rag_context.user_profile:
            profile_info: List[str] = []
            if rag_context.user_profile.name:
                profile_info.append(f"ì‚¬ìš©ì ì´ë¦„: {rag_context.user_profile.name}")
            if rag_context.user_profile.interests:
                profile_info.append(
                    f"ê´€ì‹¬ì‚¬: {', '.join(rag_context.user_profile.interests)}"
                )

            if profile_info:
                prompt_parts.append("ì‚¬ìš©ì ì •ë³´:\n" + "\n".join(profile_info))

        # ëŒ€í™” ë§¥ë½ ì •ë³´
        if (
            rag_context.conversation_context
            and rag_context.conversation_context.summary
        ):
            prompt_parts.append(
                f"ì´ì „ ëŒ€í™” ë§¥ë½: {rag_context.conversation_context.summary}"
            )

        # ê´€ë ¨ ë©”ëª¨ë¦¬ ì •ë³´
        if rag_context.relevant_memories:
            memory_info: List[str] = []
            for memory in rag_context.relevant_memories:
                memory_info.append(f"- {memory.key}: {memory.value}")

            if memory_info:
                prompt_parts.append("ê´€ë ¨ ê¸°ì–µ:\n" + "\n".join(memory_info))

        # ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ (ìš”ì•½ëœ í˜•íƒœë¡œ ì œê³µ)
        if rag_context.search_results:
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ì—¬ ì œê³µ
            summarized_results = self._summarize_search_results(
                rag_context.search_results
            )
            prompt_parts.append(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\n{summarized_results}")
        else:
            prompt_parts.append(
                "âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."
            )

        # ìœ„í‚¤ ì •ë³´
        if rag_context.wiki_summary:
            prompt_parts.append(f"ìœ„í‚¤ë°±ê³¼ ì •ë³´:\n{rag_context.wiki_summary}")

        # ì‚¬ìš©ì ì§ˆë¬¸
        prompt_parts.append(f"\nì‚¬ìš©ì ì§ˆë¬¸: {rag_context.user_query}")
        prompt_parts.append(
            "\nìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:"
        )

        return "\n\n".join(prompt_parts)

    async def _generate_llm_response(self, rag_prompt: str, request_id: str) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        try:
            provider = self.llm_service.get_provider()
            content, _ = await provider.invoke(
                model="gpt-3.5-turbo",
                system_prompt="ë‹¹ì‹ ì€ ì™¸ë¶€ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                user_prompt=rag_prompt,
                request_id=request_id,
                response_format="text",
            )
            return content
        except Exception as e:
            logger.error(f"Failed to generate LLM response: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    async def _update_context_after_rag_response(
        self,
        room_id: str,
        user_id: str,
        user_message: str,
        ai_response: str,
        rag_context: RAGContext,
    ) -> None:
        """RAG ì‘ë‹µ í›„ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        try:
            from app.models.memory_schemas import ContextUpdate

            # ê¸°ì¡´ ë§¥ë½ê³¼ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ê²°í•©í•œ ìš”ì•½ ìƒì„±
            existing_summary = ""
            if (
                rag_context.conversation_context
                and rag_context.conversation_context.summary
            ):
                existing_summary = rag_context.conversation_context.summary + " "

            # ìƒˆë¡œìš´ ëŒ€í™” ìš”ì•½ ìƒì„±
            new_summary = f"{existing_summary}ì‚¬ìš©ìê°€ '{user_message}'ì— ëŒ€í•´ ì§ˆë¬¸í–ˆê³ , AIê°€ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í–ˆìŠµë‹ˆë‹¤."

            # ì£¼ìš” ì£¼ì œ ì¶”ì¶œ (ê¸°ì¡´ + ìƒˆë¡œìš´ ì£¼ì œ ê²°í•©)
            existing_topics = []
            if (
                rag_context.conversation_context
                and rag_context.conversation_context.key_topics
            ):
                existing_topics = rag_context.conversation_context.key_topics

            new_topics = self._extract_key_topics(user_message + " " + ai_response)

            # ì¤‘ë³µ ì œê±°í•˜ê³  ê²°í•©
            all_topics = list(set(existing_topics + new_topics))[:5]  # ìµœëŒ€ 5ê°œ

            # ê°ì • ë¶„ì„
            sentiment = self._analyze_sentiment(user_message + " " + ai_response)

            # ë§¥ë½ ì—…ë°ì´íŠ¸
            context_update = ContextUpdate(
                room_id=room_id,
                user_id=user_id,
                summary=new_summary,
                key_topics=all_topics,
                sentiment=sentiment,
            )

            await self.memory_service.update_context(context_update)

        except Exception as e:
            logger.error(f"Failed to update context after RAG response: {e}")

    def _extract_key_topics(self, text: str) -> List[str]:
        """ì£¼ìš” ì£¼ì œ ì¶”ì¶œ"""
        keywords = [
            "ì‹œê°„",
            "ë‚ ì”¨",
            "ê²€ìƒ‰",
            "ìœ„í‚¤",
            "ì´ë¦„",
            "í”„ë¡œì íŠ¸",
            "ì¼ì •",
            "ìŒì‹",
            "ì˜í™”",
            "ìŒì•…",
            "ìš´ë™",
            "ê±´ê°•",
            "ì—¬í–‰",
            "ì·¨ë¯¸",
            "ì¼",
            "í•™ìŠµ",
            "ê¸°ìˆ ",
            "AI",
            "ì¸ê³µì§€ëŠ¥",
            "í”„ë¡œê·¸ë˜ë°",
            "ìµœì‹ ",
            "íŠ¸ë Œë“œ",
            "ë‰´ìŠ¤",
            "ì •ë³´",
            "ë°©ë²•",
            "ê¸°ë²•",
        ]

        found_topics: List[str] = []
        for keyword in keywords:
            if keyword in text:
                found_topics.append(keyword)

        return found_topics[:5]

    def _filter_and_rank_search_results(
        self, results: List[Dict[str, str]], query: str
    ) -> List[Dict[str, str]]:
        """ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ë° ë­í‚¹"""
        if not results:
            return []

        scored_results: List[Tuple[int, Dict[str, str]]] = []
        query_lower = query.lower()

        for result in results:
            score = 0
            title = result.get("title", "").lower()
            snippet = result.get("snippet", "").lower()
            link = result.get("link", "").lower()

            # 1. ì œëª© ê´€ë ¨ì„± ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”)
            title_words = title.split()
            query_words = query_lower.split()
            title_match = sum(
                1
                for word in query_words
                if any(word in title_word for title_word in title_words)
            )
            score += title_match * 10

            # 2. ìŠ¤ë‹ˆí« ê´€ë ¨ì„± ì ìˆ˜
            snippet_words = snippet.split()
            snippet_match = sum(
                1
                for word in query_words
                if any(word in snippet_word for snippet_word in snippet_words)
            )
            score += snippet_match * 5

            # 3. ë„ë©”ì¸ ì‹ ë¢°ì„± ì ìˆ˜
            trusted_domains = [
                "wikipedia.org",
                "stackoverflow.com",
                "github.com",
                "medium.com",
                "techcrunch.com",
                "arstechnica.com",
                "ieee.org",
                "acm.org",
                "researchgate.net",
                "arxiv.org",
                "scholar.google.com",
            ]
            domain_score = sum(3 for domain in trusted_domains if domain in link)
            score += domain_score

            # 4. ìµœì‹ ì„± ì ìˆ˜ (URLì— ì—°ë„ê°€ í¬í•¨ëœ ê²½ìš°)
            import re

            year_pattern = r"20[12]\d"  # 2010ë…„ ì´í›„
            if re.search(year_pattern, link) or re.search(year_pattern, title):
                score += 5

            # 5. ìŠ¤íŒ¸/ê´‘ê³  í•„í„°ë§
            spam_keywords = [
                "click",
                "buy",
                "discount",
                "sale",
                "advertisement",
                "sponsored",
            ]
            if any(keyword in title or keyword in snippet for keyword in spam_keywords):
                score -= 10

            # 6. ë‚´ìš© ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ê²ƒì€ ì œì™¸)
            content_length = len(title) + len(snippet)
            if 50 <= content_length <= 500:
                score += 3
            elif content_length < 20:
                score -= 5

            scored_results.append((score, result))

        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        scored_results.sort(key=lambda x: x[0], reverse=True)
        return [result for score, result in scored_results if score > 0]

    def _summarize_search_results(self, results: List[Dict[str, str]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë° ì •ë¦¬"""
        if not results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
        top_results = results[:2]

        summary_parts: List[str] = []

        for i, result in enumerate(top_results, 1):
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            link = result.get("link", "")

            # ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            title_words = title.split()
            if len(title_words) > 8:
                title = " ".join(title_words[:8]) + "..."

            # ìŠ¤ë‹ˆí« ìš”ì•½ (100ì ì´ë‚´)
            if len(snippet) > 100:
                snippet = snippet[:100] + "..."

            # ë„ë©”ì¸ ì¶”ì¶œ
            domain = self._extract_domain(link)

            summary_parts.append(
                f"{i}. {title}\n   {snippet}\n   ì¶œì²˜: {domain}"
            )

        return "\n\n".join(summary_parts)

    def _extract_domain(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse

            parsed = urlparse(url)
            domain = parsed.netloc
            # www ì œê±°
            if domain.startswith("www."):
                domain = domain[4:]
            return domain
        except Exception:
            return url[:30] + "..." if len(url) > 30 else url

    def _validate_wiki_content(self, wiki_content: str) -> str:
        """ìœ„í‚¤ ë‚´ìš© ê²€ì¦ ë° ì •ë¦¬"""
        if not wiki_content:
            return ""

        # ë„ˆë¬´ ì§§ì€ ë‚´ìš© í•„í„°ë§
        if len(wiki_content) < 50:
            return ""

        # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ì •ë¦¬)
        import re

        cleaned_content = re.sub(r"<[^>]+>", "", wiki_content)

        # ì¤‘ë³µ ê³µë°± ì œê±°
        cleaned_content = re.sub(r"\s+", " ", cleaned_content).strip()

        return cleaned_content

    def _analyze_sentiment(self, text: str) -> str:
        """ê°ì • ë¶„ì„"""
        positive_words = ["ì¢‹ì•„", "ê°ì‚¬", "í–‰ë³µ", "ì¦ê±°", "ì¬ë¯¸", "í›Œë¥­", "ì™„ë²½"]
        negative_words = ["ì‹«ì–´", "í™”ë‚˜", "ìŠ¬í”„", "ì§œì¦", "ë¶ˆë§Œ", "ì‹¤ë§", "ì–´ë ¤ì›Œ"]

        text_lower = text.lower()

        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)

        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"


